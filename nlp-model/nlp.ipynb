{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PWYu_6CRqKZo",
        "outputId": "f3b5e092-5941-4507-e97a-2b1b9388f59c"
      },
      "source": [
        "!pip install textract"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textract\n",
            "  Downloading https://files.pythonhosted.org/packages/32/31/ef9451e6e48a1a57e337c5f20d4ef58c1a13d91560d2574c738b1320bb8d/textract-1.6.3-py3-none-any.whl\n",
            "Collecting SpeechRecognition==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8MB 88kB/s \n",
            "\u001b[?25hCollecting python-pptx==0.6.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/86/eb979f7b0333ec769041aae36df8b9f1bd8bea5bbad44620663890dce561/python-pptx-0.6.18.tar.gz (8.9MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9MB 17.9MB/s \n",
            "\u001b[?25hCollecting argcomplete==1.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/4d/82/f44c9661e479207348a979b1f6f063625d11dc4ca6256af053719bbb0124/argcomplete-1.10.0-py2.py3-none-any.whl\n",
            "Collecting docx2txt==0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/7d/60ee3f2b16d9bfdfa72e8599470a2c1a5b759cb113c6fe1006be28359327/docx2txt-0.8.tar.gz\n",
            "Collecting EbookLib==0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/38/7d6ab2e569a9165249619d73b7bc6be0e713a899a3bc2513814b6598a84c/EbookLib-0.17.1.tar.gz (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 37.3MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/b7/34eec2fe5a49718944e215fde81288eec1fa04638aa3fb57c1c6cd0f98c3/beautifulsoup4-4.8.0-py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.8MB/s \n",
            "\u001b[?25hCollecting xlrd==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/16/63576a1a001752e34bf8ea62e367997530dc553b689356b9879339cf45a4/xlrd-1.2.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 39.9MB/s \n",
            "\u001b[?25hCollecting pdfminer.six==20181108\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/fd/6e8746e6965d1a7ea8e97253e3d79e625da5547e8f376f88de5d024bacb9/pdfminer.six-20181108-py2.py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 14.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from textract) (3.0.4)\n",
            "Collecting six==1.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting extract-msg==0.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/90/84485a914ed90adb5e87df17e626be04162fbba146dfecf34643659a4633/extract_msg-0.23.1-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from python-pptx==0.6.18->textract) (4.2.6)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from python-pptx==0.6.18->textract) (7.1.2)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/ce/74fd8d638a5b82ea0c6f08a5978f741c2655a38c3d6e82f73a0f084377e6/XlsxWriter-1.4.3-py2.py3-none-any.whl (149kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 46.6MB/s \n",
            "\u001b[?25hCollecting soupsieve>=1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20181108->textract) (2.4.0)\n",
            "Collecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/16/9627ab0493894a11c68e46000dbcc82f578c8ff06bc2980dcd016aea9bd3/pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 30.1MB/s \n",
            "\u001b[?25hCollecting olefile==0.46\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/81/e1ac43c6b45b4c5f8d9352396a14144bba52c8fec72a80f425f6a4d653ad/olefile-0.46.zip (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 45.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal==1.5.1 in /usr/local/lib/python3.7/dist-packages (from extract-msg==0.23.1->textract) (1.5.1)\n",
            "Collecting imapclient==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/39/e1c2c2c6e2356ab6ea81fcfc0a74b044b311d6a91a45300811d9a6077ef7/IMAPClient-2.1.0-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tzlocal==1.5.1->extract-msg==0.23.1->textract) (2018.9)\n",
            "Building wheels for collected packages: python-pptx, docx2txt, EbookLib, olefile\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.18-cp37-none-any.whl size=275504 sha256=271f08ce2a526741bb5ed592263f665da24c2809d9716cd0cb7e4fd01d9b36df\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/1f/2c/29acca422b420a0b5210bd2cd7e9669804520d602d2462f20b\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-cp37-none-any.whl size=3981 sha256=5db4c32cd30bc52a8fd21b98525aa897a6b0342c2b65d929cf5b7487d88a9888\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/1f/26/a051209bbb77fc6bcfae2bb7e01fa0ff941b82292ab084d596\n",
            "  Building wheel for EbookLib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for EbookLib: filename=EbookLib-0.17.1-cp37-none-any.whl size=38187 sha256=91a8f3d52cf950de6be660a61daee217e0e092c1a04ac587b3dbdeb078f02306\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/11/01/951369cbbf8f96878786a1f4da68bd7ac19a5d945b38e03d54\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35431 sha256=ea55e544cf9e436fe5b02dbae4aeeb3297f5d09c5b3a29a602d612274290081c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/f4/11/bc4166107c27f07fd7bba707ffcb439619197638a1ac986df3\n",
            "Successfully built python-pptx docx2txt EbookLib olefile\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-core 1.26.3 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: SpeechRecognition, XlsxWriter, python-pptx, argcomplete, docx2txt, six, EbookLib, soupsieve, beautifulsoup4, xlrd, pycryptodome, pdfminer.six, olefile, imapclient, extract-msg, textract\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "Successfully installed EbookLib-0.17.1 SpeechRecognition-3.8.1 XlsxWriter-1.4.3 argcomplete-1.10.0 beautifulsoup4-4.8.0 docx2txt-0.8 extract-msg-0.23.1 imapclient-2.1.0 olefile-0.46 pdfminer.six-20181108 pycryptodome-3.10.1 python-pptx-0.6.18 six-1.12.0 soupsieve-2.2.1 textract-1.6.3 xlrd-1.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmpEkxDS90am",
        "outputId": "d8638e58-d5dd-4144-9a5b-67e5de9f175a"
      },
      "source": [
        "%cd drive/MyDrive/GEP"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/GEP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5liU5oakqaB6",
        "outputId": "3c3548e9-8457-4b59-c8c6-b13c96d1cd65"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "import collections\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from joblib import dump, load\n",
        "import pickle\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from itertools import chain\n",
        "import textract\n",
        "from gensim.models import Word2Vec\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gensim\n",
        "from gensim.models.phrases import Phraser, Phrases\n",
        "import nltk\n",
        "import collections\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UtZKvgX_9qu"
      },
      "source": [
        "\n",
        "def cleanResume(resumeText):\n",
        "    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  \n",
        "    resumeText = re.sub('RT|cc', ' ', resumeText)  \n",
        "    resumeText = re.sub('#\\S+', '', resumeText) \n",
        "    resumeText = re.sub('@\\S+', '  ', resumeText) \n",
        "    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)\n",
        "    resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText) \n",
        "    resumeText = re.sub('\\s+', ' ', resumeText)  \n",
        "    return resumeText"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnmbLA8WrJXy"
      },
      "source": [
        "def Preprocessfile(filename):\n",
        "  text = textract.process(filename)\n",
        "  text= text.decode('utf-8').replace(\"\\\\n\", \" \")\n",
        "  # print(text)\n",
        "  x=[]\n",
        "  tokens=word_tokenize(text)\n",
        "  tok=[w.lower() for w in tokens]\n",
        "  table=str.maketrans('','',string.punctuation)\n",
        "  strpp=[w.translate(table) for w in tok]\n",
        "  words=[word for word in strpp if word.isalpha()]\n",
        "  stop_words=set(stopwords.words('english'))\n",
        "  words=[w for w in words if not w in stop_words]\n",
        "  x.append(words)\n",
        "  # print(x)\n",
        "  res=\" \".join(chain.from_iterable(x))\n",
        "  return res"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sMV-BEBsD5R",
        "outputId": "4f8b492b-d4b7-4026-dbbc-dafb35c62cf3"
      },
      "source": [
        "x=Preprocessfile('jobtype.txt')\n",
        "y=Preprocessfile('resume.pdf')\n",
        "text=[y,x]\n",
        "\n",
        "\n",
        "\n",
        "#print the similarity score\n",
        "print(\"\\n Similarity Score: \")\n",
        "cv = CountVectorizer()\n",
        "count_matrix = cv.fit_transform(text)\n",
        "print(cosine_similarity(count_matrix))\n",
        "matchpercent = cosine_similarity(count_matrix)[0][1]*100\n",
        "matchpercent = round(matchpercent,2)\n",
        "\n",
        "print(\"Your Resume matches about \" + str(matchpercent) + \"% of the job\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Similarity Score: \n",
            "[[1.         0.51460293]\n",
            " [0.51460293 1.        ]]\n",
            "Your Resume matches about 51.46% of the job\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MheyHWX92P56"
      },
      "source": [
        "#all custom keywords should be in lower case\n",
        "def find_score(jobdes,filename,customKeywords):   \n",
        "    resume=Preprocessfile(filename)\n",
        "    customKeywords = ' '.join(customKeywords)\n",
        "    jobdes=jobdes + ' ' + customKeywords\n",
        "    text=[resume,jobdes]\n",
        "    cv = CountVectorizer()\n",
        "    count_matrix = cv.fit_transform(text)\n",
        "    print(cosine_similarity(count_matrix))\n",
        "    matchpercent = cosine_similarity(count_matrix)[0][1]*100\n",
        "    matchpercent = round(matchpercent,2)\n",
        "    print(matchpercent)\n",
        "    return matchpercent"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZM_fIInFC9B"
      },
      "source": [
        "\n",
        "def predictResume(filename):\n",
        "  text = textract.process(filename)\n",
        "  text= text.decode('utf-8').replace(\"\\\\n\", \" \")\n",
        "  text=cleanResume(text)\n",
        "  text=[text]\n",
        "  text=np.array(text)\n",
        "  vectorizer = pickle.load(open(\"vectorizer.pickle\", \"rb\"))\n",
        "  resume = vectorizer.transform(text)\n",
        "  model = load('model.joblib') \n",
        "  result=model.predict(resume)\n",
        "  labeldict={\n",
        "    0:'Arts',\n",
        "    1:'Automation Testing',\n",
        "    2:'Operations Manager',\n",
        "    3:'DotNet Developer',\n",
        "    4:'Civil Engineer',\n",
        "    5:'Data Science',\n",
        "    6:'Database',\n",
        "    7:'DevOps Engineer',\n",
        "    8:'Business Analyst',\n",
        "    9:'Health and fitness',\n",
        "    10:'HR',\n",
        "    11:'Electrical Engineering',\n",
        "    12:'Java Developer',\n",
        "    13:'Mechanical Engineer',\n",
        "    14:'Network Security Engineer',\n",
        "    15:'Blockchain ',\n",
        "    16:'Python Developer',\n",
        "    17:'Sales',\n",
        "    18:'Testing',\n",
        "    19:'Web Designing'\n",
        "  }\n",
        "  return labeldict[result[0]]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwgMl1_zs48H",
        "outputId": "0c9c2140-dc9b-4aa9-b6a8-238710a435fa"
      },
      "source": [
        "#FLOW\n",
        "\n",
        "profile_type=[] # user inputs profile type from the abive 20 options multiple profiles are possible\n",
        "\n",
        "resume='resumeds.pdf'\n",
        "\n",
        "#iterate through all resumes here\n",
        "predictedprofile=predictResume(resume)\n",
        "print(predictedprofile)   #classifies what type of resume is inputed, if it from the mentioned profile_type keep other wise disgard\n",
        "\n",
        "#ask user to enter job des\n",
        "jobdes=Preprocessfile('dsjobdes.txt')  #the job description is preprocessed outside as the same job description is used for the multiple resumes\n",
        "\n",
        "#askuser to enter custom keywords\n",
        "customKeywords=['spanish','hindi','opencv']\n",
        "\n",
        "\n",
        "#iterate through all resumes here\n",
        "results=find_score(jobdes,resume,customKeywords);\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Science\n",
            "[[1.         0.51409916]\n",
            " [0.51409916 1.        ]]\n",
            "51.41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HocfiUgZ_gmn",
        "outputId": "9102c73b-4636-4597-df5a-19724e85c08d"
      },
      "source": [
        "result=predictResume('resume.pdf')\n",
        "print(result)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Science\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFSIYI09_owI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}